import argparse
import pathlib
from conf import default, general, paths
import os
import numpy as np
from sklearn.metrics import f1_score, confusion_matrix
from skimage.morphology import area_opening
import pandas as pd
from itertools import repeat
from multiprocessing import Pool, freeze_support
import matplotlib.pyplot as plt
from tqdm import tqdm

parser = argparse.ArgumentParser(
    description='Evaluate F1-Score of the models\' predictions'
)

parser.add_argument( # Experiment number
    '-e', '--experiment',
    type = int,
    default = 1,
    help = 'The number of the experiment'
)


parser.add_argument( # Number of models to be trained
    '-n', '--number-models',
    type = int,
    default = default.N_TRAIN_MODELS,
    help = 'The number models to be trained from the scratch'
)

parser.add_argument( # Experiment path
    '-x', '--experiments-path',
    type = pathlib.Path,
    default = paths.EXPERIMENTS_PATH,
    help = 'The patch to data generated by all experiments'
)

args = parser.parse_args()

exp_path = os.path.join(str(args.experiments_path), f'exp_{args.experiment}')
if not os.path.exists(exp_path):
    os.mkdir(exp_path)

logs_path = os.path.join(exp_path, f'logs')
models_path = os.path.join(exp_path, f'models')
visual_path = os.path.join(exp_path, f'visual')
predicted_path = os.path.join(exp_path, f'predicted')
results_path = os.path.join(exp_path, f'results')
'''
preds = []
for model_idx in tqdm(range(args.number_models), desc = 'Opening prediction files'):
    preds.append(np.load(os.path.join(predicted_path, f'pred_{model_idx}.npy')))
preds = np.array(preds)
np.save(os.path.join(predicted_path, 'pred_m') ,preds.mean(axis=0))

del preds
'''
preds_files = os.listdir(predicted_path)
label = np.load(os.path.join(paths.PREPARED_PATH, f'{general.LABEL_PREFIX}_{general.YEAR_2}.npy'))
tns = []
fps = []
fns = []
tps = []
f1s = []
precs = []
recs = []
accs = []
exps = []

for pred_file in tqdm(preds_files, desc = 'Generating F1-Score'):
    
    label_b = label.copy()
    pred_idx = pred_file.split('_')[1].split('.')[0]
    exps.append(f'Exp {pred_idx}')
    pred_prob = np.load(os.path.join(predicted_path, pred_file))[:,:,1]

    pred_b = np.zeros_like(pred_prob, dtype=np.uint8)
    pred_b[pred_prob >= 0.5] = 1
    pred_removed = pred_b -  area_opening(pred_b, 625)
    label_b[pred_removed == 1] = 2

    keep_idx = label_b.flatten() != 2

    pred_eval = pred_b.flatten()[keep_idx]
    label_eval = label_b.flatten()[keep_idx]

    tn, fp, fn, tp = confusion_matrix(label_eval, pred_eval).ravel()
    tns.append(tn)
    fps.append(fp)
    fns.append(fn)
    tps.append(tp)

    prec = tp/(tp+fp)
    precs.append(prec)
    
    rec = tp/(tp+fn)
    recs.append(rec)

    acc = (tp+tn)/(tp+fn+tn+fp)
    accs.append(acc)

    f1 = 2 * prec * rec /(prec + rec)
    f1s.append(f1)

pd.DataFrame({
        'Exp': exps,
        'F1 Score': f1s,
        'Precision': precs,
        'Recall': recs,
        'Accuracy':accs,
        'TP':tps,
        'FP':fps,
        'TN':tns,
        'FN':fns
    }).to_excel(os.path.join(results_path, f'f1score_{args.experiment}.xlsx'))

